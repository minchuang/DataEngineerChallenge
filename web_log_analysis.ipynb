{"cells":[{"cell_type":"code","source":["# https://github.com/Pay-Baymax/DataEngineerChallenge\n\n# # DataEngineerChallenge\n\n# This is an interview challenge for PayPay. Please feel free to fork. Pull Requests will be ignored.\n# The challenge is to make make analytical observations about the data using the distributed tools below.\n\n# ## Processing & Analytical goals:\n# 1. Sessionize the web log by IP. Sessionize = aggregrate all page hits by visitor/IP during a session.\n#     https://en.wikipedia.org/wiki/Session_(web_analytics)\n# 2. Determine the average session time\n# 3. Determine unique URL visits per session. To clarify, count a hit to a unique URL only once per session.\n# 4. Find the most engaged users, ie the IPs with the longest session times\n\n# ## Additional questions for Machine Learning Engineer (MLE) candidates:\n# 1. Predict the expected load (requests/second) in the next minute\n# 2. Predict the session length for a given IP\n# 3. Predict the number of unique URL visits by a given IP\n\n# ## Tools allowed (in no particular order):\n# - Spark (any language, but prefer Scala or Java)\n# - Pig\n# - MapReduce (Hadoop 2.x only)\n# - Flink\n# - Cascading, Cascalog, or Scalding\n\n# If you need Hadoop, we suggest \n# HDP Sandbox:\n# http://hortonworks.com/hdp/downloads/\n# or \n# CDH QuickStart VM:\n# http://www.cloudera.com/content/cloudera/en/downloads.html\n\n\n# ### Additional notes:\n# - You are allowed to use whatever libraries/parsers/solutions you can find provided you can explain the functions you are implementing in detail.\n# - IP addresses do not guarantee distinct users, but this is the limitation of the data. As a bonus, consider what additional data would help make better analytical conclusions\n# - For this dataset, complete the sessionization by time window rather than navigation. Feel free to determine the best session window time on your own, or start with 15 minutes.\n# - The log file was taken from an AWS Elastic Load Balancer:\n# http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/access-log-collection.html#access-log-entry-format\n\n# ## How to complete this challenge:\n# A. Fork this repo in github\n# B. Complete the processing and analytics as defined first to the best of your ability with the time provided.\n# C. Place notes in your code to help with clarity where appropriate. Make it readable enough to present to the PayPay interview team.\n# D. Complete your work in your own github repo and send the results to us and/or present them during your interview.\n# ## What are we looking for? What does this prove?\n\n# We want to see how you handle:\n# - New technologies and frameworks\n# - Messy (ie real) data\n# - Understanding data transformation\n# This is not a pass or fail test, we want to hear about your challenges and your successes with this particular problem."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["#---------------------------------------\n#   PGM     : web log analysis\n#   author  : min\n#   create  : 2019/04/20\n#   lang    : pyspark\n#   func    : analyze the elb web log\n#   history : 2019/04/20 new @min\n#---------------------------------------\n\n\"\"\"\nAnalysis of aws elb web log.\nUse:\n  - databricks spark env\n  - databricks notebook\n  - pyspark\nFlow:\n  - Parse elb web logs.\n  - Sessionize.\n  - Analyze session time.\n  - Count unique requests.\n  - Find longest session times.\n\"\"\"\n\nfrom pyspark.sql import SparkSession\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag, max, sum, mean\nfrom pyspark.sql.functions import col, when, count, countDistinct\nfrom pyspark.sql.functions import split, concat_ws\n\nfrom pyspark.sql.types import StructField, StructType\nfrom pyspark.sql.types import StringType, FloatType\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["def parse_logs(spark, logFile, numPartitions=10):\n    \"\"\"define schema from aws elb\n    aws elb web Log format\n    https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html\n\n    datatype\n    https://spark.apache.org/docs/latest/sql-reference.html\n    StructField(fieldName, StringType, nullable = true)\"\"\"\n    log_schema = StructType([\n        StructField(\"timestamp\", StringType(), False),\n        StructField(\"elb\", StringType(), False),\n        StructField(\"client:port\", StringType(), False),\n        StructField(\"backend:port\", StringType(), False),\n        StructField(\"request_processing_time\", StringType(), False),\n        StructField(\"backend_processing_time\", StringType(), False),\n        StructField(\"response_processing_time\", StringType(), False),\n        StructField(\"elb_status_code\", StringType(), False),\n        StructField(\"backend_status_code\", StringType(), False),\n        StructField(\"received_bytes\", StringType(), False),\n        StructField(\"sent_bytes\", StringType(), False),\n        StructField(\"request\", StringType(), False),\n        StructField(\"user_agent\", StringType(), False),\n        StructField(\"ssl_cipher\", StringType(), False),\n        StructField(\"ssl_protocol\", StringType(), False)])\n    #\n    # load elb log data\n    # repatitions and cache the df\n    init_df = spark.read.csv(logFile, schema=log_schema, sep=\" \").repartition(numPartitions).cache()\n    # print (init_df.rdd.getNumPartitions())\n    # \n    # etl access data\n    split_clinet = split(init_df[\"client:port\"], \":\")\n    split_backend = split(init_df[\"backend:port\"], \":\")\n    split_request = split(init_df[\"request\"], \" \")\n    # \n    df = init_df.withColumn(\"client_ip\", split_clinet.getItem(0)) \\\n                .withColumn(\"client_port\", split_clinet.getItem(1)) \\\n                .withColumn(\"backend_ip\", split_backend.getItem(0)) \\\n                .withColumn(\"backend_port\", split_backend.getItem(1)) \\\n                .withColumn(\"request_action\", split_request.getItem(0)) \\\n                .withColumn(\"request_url\", split_request.getItem(1)) \\\n                .withColumn(\"request_protocol\", split_request.getItem(2)) \\\n                .withColumn(\"curr_timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n                .drop(\"client:port\",\"backend:port\",\"request\").cache()\n    # df.show(5)\n    return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["def time_diff(start, end):\n    \"\"\"\n    get diff between start and end\n    \"\"\"\n    try:\n        diff_secs = (end - start).total_seconds()\n    except:\n        diff_secs = 0\n    return diff_secs\nget_time_diff = udf(time_diff, FloatType())\n\n\ndef Sessionize(df_logs, session_time=15):\n    \"\"\"\n    Sessionize the web log by IP.\n    Sessionize = aggregrate all page hits by visitor/IP during a session.\n    https://en.wikipedia.org/wiki/Session_(web_analytics)\n    \"\"\"\n    session_time_secs = session_time * 60\n    window_func_ip = Window.partitionBy(\"client_ip\").orderBy(\"curr_timestamp\")\n    df = df_logs.withColumn(\"prev_timestamp\", \n                            lag(col(\"curr_timestamp\")).over(window_func_ip)) \\\n                .withColumn(\"session_lasts\", \n                            get_time_diff(col(\"prev_timestamp\"), col(\"curr_timestamp\"))) \\\n                .withColumn(\"new_session_flag\", \n                            when((col(\"session_lasts\") > session_time_secs), 1).otherwise(0)) \\\n                .withColumn(\"count_session\", \n                            sum(col(\"new_session_flag\")).over(window_func_ip)) \\\n                .withColumn(\"ip_session_count\", \n                            concat_ws(\"_\", col(\"client_ip\"), col(\"count_session\")))\n    \n    df_ip_session = df.select([\"ip_session_count\", \"client_ip\", \"request_url\",\n                               \"prev_timestamp\", \"curr_timestamp\", \n                               \"session_lasts\", \"new_session_flag\", \"count_session\"])\n    return df_ip_session"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["def analyze_session_time(df_ip_session, save_res_to_csv=False):\n    \"\"\"\n    Determine the average session time.\n    \"\"\"\n    window_func_session = Window.partitionBy(\"ip_session_count\").orderBy(\"curr_timestamp\")\n    df_session = df_ip_session.withColumn(\"prev_timestamp_session\", \n                              lag(df_ip_session[\"curr_timestamp\"]).over(window_func_session)) \\\n                  .withColumn(\"current_session_lasts\", \n                              get_time_diff(col(\"prev_timestamp_session\"), col(\"curr_timestamp\")))\n    #\n    df_session_total = df_session.groupby(\"ip_session_count\").agg(\n            sum(\"current_session_lasts\").alias(\"total_session_time\")).cache()\n    # Average session time.\n    df_session_avg = df_session_total.select([mean(\"total_session_time\").alias(\"avg_session_time\")]).cache()\n    if save_res_to_csv:\n        df_session_avg.coalesce(1).write.csv(path=\"df_session_avg.csv\", header=True, sep=\",\", mode=\"overwrite\")\n    df_session_avg.show()\n    #\n    return df_session"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["def count_unique_request(df_session):\n    \"\"\"\n    Determine unique URL visits per session.\n    To clarify, count a hit to a unique URL only once per session.\n    \"\"\"\n    df_unique_url = df_session.groupby(\"ip_session_count\").agg(\n        countDistinct(\"request_url\").alias(\"count_unique_requests\"))\n    df_unique_url.show()\n    return df_unique_url\n\ndef get_longest_session_time(df_session):\n    \"\"\"\n    get the longest session time\n    \"\"\"\n    df_ip_time = df_session.groupby(\"client_ip\").agg(\n                sum(\"session_lasts\").alias(\"session_time_all\"),\n                count(\"client_ip\").alias(\"num_sessions\"),\n                max(\"session_lasts\").alias(\"session_lasts_max\")) \\\n          .withColumn(\"avg_session_time\", col(\"session_time_all\") / col(\"num_sessions\")) \\\n          .orderBy(col(\"avg_session_time\"), ascending=False)\n    #\n    df_ip_time.show()\n    return df_ip_time"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["def exec_analysis(spark, logFile, numPartitions=10, session_time=15, save_res_to_csv=False):\n    \"\"\"\n    Find the most engaged users, ie the IPs with the longest session times\n    execute the analysis.\n    \"\"\"\n    # parese the logs by regex -> slow\n    # parse the logs\n    df_logs = parse_logs(spark, logFile, numPartitions).repartition(numPartitions).cache()\n    df_ip_session = Sessionize(df_logs, session_time).repartition(numPartitions).cache()\n    df_session = analyze_session_time(df_ip_session, save_res_to_csv).repartition(numPartitions).cache()\n    df_unique_url = count_unique_request(df_session).cache()\n    df_ip_time = get_longest_session_time(df_session).cache()\n    if save_res_to_csv:\n        df_session.coalesce(1).write.csv(path=\"df_session.csv\", header=True, sep=\",\", mode=\"overwrite\")\n        df_unique_url.coalesce(1).write.csv(path=\"df_unique_url.csv\", header=True, sep=\",\", mode=\"overwrite\")\n        df_ip_time.coalesce(1).write.csv(path=\"df_ip_time.csv\", header=True, sep=\",\", mode=\"overwrite\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["##############main######################\nif __name__ == \"__main__\":\n    if \"spark\" not in dir():\n      spark = SparkSession.builder \\\n        .appName(\"web_log_analysis\") \\\n        .getOrCreate()\n    #\n    logFile = \"/FileStore/tables/2015_07_22_mktplace_shop_web_log_sample_log-214a9.gz\"\n    numPartitions = 10\n    session_time = 15\n    save_res_to_csv = True\n    #\n    exec_analysis(spark, logFile, numPartitions, session_time, save_res_to_csv)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+\n  avg_session_time|\n+------------------+\n125.63083815583757|\n+------------------+\n\n+-----------------+---------------------+\n ip_session_count|count_unique_requests|\n+-----------------+---------------------+\n115.248.233.203_2|                   86|\n 59.165.251.191_2|                   86|\n117.239.224.160_1|                   65|\n205.175.226.101_0|                   89|\n    8.37.228.47_1|                   69|\n 115.249.21.130_0|                   10|\n   202.91.134.7_4|                   10|\n 122.164.34.125_0|                    8|\n  182.68.136.65_0|                  104|\n115.242.129.233_0|                    7|\n 223.255.247.66_0|                    7|\n 59.184.184.157_0|                    9|\n  188.40.94.195_1|                   89|\n   182.69.48.36_0|                  108|\n 117.210.14.119_0|                    3|\n  192.193.164.9_1|                   55|\n  101.57.193.44_0|                   82|\n122.179.135.178_0|                    8|\n    1.39.61.253_0|                   59|\n 115.111.50.254_1|                   18|\n+-----------------+---------------------+\nonly showing top 20 rows\n\n+---------------+----------------+------------+-----------------+----------------+\n      client_ip|session_time_all|num_sessions|session_lasts_max|avg_session_time|\n+---------------+----------------+------------+-----------------+----------------+\n   27.120.106.3|   66298.9140625|           2|        66298.914|  33149.45703125|\n117.255.253.155|     57422.78125|           2|         57422.78|    28711.390625|\n     1.38.21.92|  54168.50390625|           2|        54168.504| 27084.251953125|\n 163.53.203.235|   54068.0390625|           2|         54068.04|  27034.01953125|\n   66.249.71.10|   53818.8046875|           2|        53818.805|  26909.40234375|\n    1.38.22.103|  50599.78515625|           2|        50599.785| 25299.892578125|\n 167.114.100.25|  50401.54296875|           2|        50401.543| 25200.771484375|\n    75.98.9.249|   49283.2578125|           2|        49283.258|  24641.62890625|\n107.167.112.248|      49079.5625|           2|        49079.562|     24539.78125|\n 168.235.200.74|   48446.5703125|           2|         48446.57|  24223.28515625|\n 122.174.94.202|     48349.15625|           2|        48349.156|    24174.578125|\n 117.253.108.44|   46465.7421875|           2|        46465.742|  23232.87109375|\n 117.244.25.135|  46324.63671875|           2|        46324.637| 23162.318359375|\n  182.75.33.150|  46251.28515625|           2|        46251.285| 23125.642578125|\n    8.37.225.38|   46245.2734375|           2|        46245.273|  23122.63671875|\n      1.38.13.1|    46214.453125|           2|        46214.453|   23107.2265625|\n    1.39.61.171|   46112.8359375|           2|        46112.836|  23056.41796875|\n  49.156.86.219|  45112.31640625|           2|        45112.316| 22556.158203125|\n 199.190.46.117|  45029.84765625|           2|        45029.848| 22514.923828125|\n   122.15.56.59|  44929.15234375|           2|        44929.152| 22464.576171875|\n+---------------+----------------+------------+-----------------+----------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":8}],"metadata":{"name":"pay","notebookId":2285115648984550},"nbformat":4,"nbformat_minor":0}
